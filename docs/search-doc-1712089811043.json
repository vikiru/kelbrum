{"searchDocs":[{"title":"✨ Acknowledgments","type":0,"sectionRef":"#","url":"/kelbrum/acknowledgments/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"✨ Acknowledgments​","type":1,"pageTitle":"✨ Acknowledgments","url":"/kelbrum/acknowledgments/#-acknowledgments","content":" csv-parsePapaParselodashlemmatizerMiniSearchReact Infinite Scrollerreact-slickremove-stopwordsslick-carouseltailwind-scrollbarword-listSimpleIconsTensorflow.jsTensorflow.js DocumentationMachine Learning Crash Course by Google Clustering AlgorithmsNormalizationMachine Learning GlossaryTransforming Categorical Data FirebaseDocusaurusGitHub PagesShields Badgesregex101Favicon Generator  Various web articles for research and learning, such as:  17 types of similarity and dissimilarity measures used in data scienceA Guide to Content-Based Filtering In Recommender SystemsGower's DistanceIntroduction to similarity metricsTypes of recommendation systems &amp; their use casesSupervised vs. Unsupervised Learning: What’s the Difference?What is unsupervised learning?  Additionally, this project would not be possible without the following sources of information:  Original Kaggle DatasetJikanAPIMyAnimeList  All external images and text used within this app belong to their respective owners. ","version":"Next","tagName":"h2"},{"title":"🔥 Motivation","type":0,"sectionRef":"#","url":"/kelbrum/motivation/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"🔥 Motivation​","type":1,"pageTitle":"🔥 Motivation","url":"/kelbrum/motivation/#-motivation","content":" As someone who understands the challenge of discovering a new series to watch after completing another, I have always envisioned creating a tool akin to an AI which would be able to understand a users preferences given their past favourites or genre preferences as input and return series that they may like based on their similarity to the input data.  My objective with this project was to develop a tool designed to alleviate the challenge of finding new content to watch, aiming to introduce users to recommendations that align with their tastes while also exploring new or previously unknown options. Through this project, I wanted to take a step towards learning about machine learning and recommendation systems while also improving my frontend capabilities by using a new UI library alongside TailwindCSS.  The project, while not flawless in its current iteration, presents numerous opportunities for enhancement, particularly in the recommendation algorithm and the app's user interface. There are several features that I would like to implement in the future such as a hybrid recommendation system that combines collaborative filtering with the existing content-based filtering method, extending the functionality of the model to handle other content types such as manga, manhwa, and manhua, among other features. ","version":"Next","tagName":"h2"},{"title":"🌟 Features","type":0,"sectionRef":"#","url":"/kelbrum/features/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"🌟 Features​","type":1,"pageTitle":"🌟 Features","url":"/kelbrum/features/#-features","content":" The ability to search for any anime within the existing dataset via a search barA homepage featuring a hero section that encourages users to search for an anime and displays 10 anime randomly selected that meet a minimum average score, providing users with immediate recommendationsThe ability to view all anime grouped together based on properties such as genres, studios, seasons, etcThe ability to view the top 100 anime within the existing set of anime, based on average scoreA dedicated anime details page that enables users to view detailed information about an anime and receive recommendations based on similarityThe ability to view 10 unique random anime recommendations and view up to 200 recommendations per anime (not all anime will have that many recommendations)The ability to grow and accommodate other content types such as manga, manhwa, and manhuaThe capability to prioritize anime properties based on assigned weights and adjust the recommendation algorithm at any time using the provided K-means JSON file and feature tensors ","version":"Next","tagName":"h2"},{"title":"Data Clustering with K-means","type":0,"sectionRef":"#","url":"/kelbrum/kmeans/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"Data Clustering with K-means​","type":1,"pageTitle":"Data Clustering with K-means","url":"/kelbrum/kmeans/#data-clustering-with-k-means","content":" ","version":"Next","tagName":"h2"},{"title":"Identifying Clustering Algorithms​","type":1,"pageTitle":"Data Clustering with K-means","url":"/kelbrum/kmeans/#identifying-clustering-algorithms","content":" Once the data was normalized, the next step was to figure out how to group the data based on their similarity. Upon researching this topic, I found out that there were several algorithms to achieve this such as:  K-means clusteringK-mediods clusteringK-nearest neighborsHierarchal clusteringDBSCAN  There were several other algorithms as well, however, I decided to use K-means clustering as I felt it was easier to understand and applicable for my use case.  ","version":"Next","tagName":"h3"},{"title":"Choosing the Right Tools​","type":1,"pageTitle":"Data Clustering with K-means","url":"/kelbrum/kmeans/#choosing-the-right-tools","content":" To cluster the data, several k-means clustering npm packages were looked at and finally, I finalized on using ml-kmeans combining it with ml-distance and simple-statistics. Addtionally, to perform TF-IDF analysis on anime synopses, natural was used alongside remove-stopwords, word-list, and lemmatizer.  ml-distance offered various distance and similarity calculations and ml-kmeans allowed for the use of custom distance functions so this worked out really well for me.  ","version":"Next","tagName":"h3"},{"title":"Understanding Distance Functions​","type":1,"pageTitle":"Data Clustering with K-means","url":"/kelbrum/kmeans/#understanding-distance-functions","content":" I tried to learn about some common distance functions used to cluster data such as euclidean, cosine, squared euclidean, manhattan, hamming, etc and through trial and error I experimented with all of the functions provided by ml-distance. Through my experiments, which involved applying various normalization techniques to anime properties and experimenting with different feature combinations, I discovered that the curse of dimensionality significantly impacted my results as the number of features in my feature tensors increased.  To combat the curse of dimensionality, I decided to use various normalization techniques, combinations of features and different weightings to these features. Additionally, I did try to reduce the number of features I used as originally, I wanted to see how the effect of all features together would be and it proved to be computationally expensive to compute for higher k values and harder to cluster effectively.  ","version":"Next","tagName":"h3"},{"title":"Evaluating Clustering Effectiveness​","type":1,"pageTitle":"Data Clustering with K-means","url":"/kelbrum/kmeans/#evaluating-clustering-effectiveness","content":" I was able to assess the effectiveness of clustering by identifying several metrics that indicate the efficiency of clustering, including:  Within Cluster Sum of Squares (WCSS)Elbow MethodSilhouette Score  While other metrics were available, my primary focus was on the Within Cluster Sum of Squares (WCSS) and the silhouette score when feasible. Fortunately, calculating the WCSS was straightforward with the assistance of ml-kmeans, and the silhouette score was computed using simple-statistics. However, the computation of the silhouette score became increasingly computationally intensive as the value of k increased, due to the large size of the feature tensors, leading me to prioritize the WCSS. My objective was to achieve the lowest WCSS and the highest silhouette score possible.  ","version":"Next","tagName":"h3"},{"title":"Best Distance Functions for the Dataset​","type":1,"pageTitle":"Data Clustering with K-means","url":"/kelbrum/kmeans/#best-distance-functions-for-the-dataset","content":" From my experiments, I learned that given my data set, the following functions worked out the best:  Manhattan DistanceDice SimilarityJaccard IndexGower's DistanceCosine SimilaritySørensen–Dice coefficientTanimoto Index  ","version":"Next","tagName":"h3"},{"title":"Customizing the Distance Function​","type":1,"pageTitle":"Data Clustering with K-means","url":"/kelbrum/kmeans/#customizing-the-distance-function","content":" After additional experimentation, I attempted to develop a custom distance function by employing varying weights and combinations of distance measures for both categorical and numerical attributes. Initially, cosine similarity was employed, as it proved effective. After cosine similarity, I ended up using gower's distance and that seemed to work out well purely based on the wcss values however, I learned later that this was due to the total number of values as the gower distance performed poorly as the number of values in the concatenated tensors for each anime increased, especially due to the amount of binary vectors.  To develop the current weighted distance function, I compared anime that were known to be similar, utilizing various distance measures with both the concatenated tensors as a whole and each individual feature tensor. This approach helped identify which properties most significantly increased the distance between each anime. At the same time, I was experimenting with different normalization techniques and feature tensor combinations. Eventually, I added weights which required a considerable amount of trial and error to achieve the current satisfactory level of recommendations as I had to experiment with different weightings for each property and different distance measures for each.  In the end, the manhattan distance was used for properties such as type, rating, and demographics where there was a numerical value and a need to seperate anime such as anime of type TV vs. Movie. The dice distance was used for all other properties such as genres, themes, synopsis, etc.  ","version":"Next","tagName":"h3"},{"title":"Future Experimentation​","type":1,"pageTitle":"Data Clustering with K-means","url":"/kelbrum/kmeans/#future-experimentation","content":" The K-means model currently uses a low k-value, k = 10 which means that there are only 10 clusters and given the dataset size, that amounts to a significant amount of anime per cluster. Further experimentation is required to investigate larger k-values, which can better balance the system's objectives and enhance the quality of recommendations. Moreover, the weights applied and the properties contain potential for further enhancement. Ideally, aiming for a larger number of clusters would enable a more accurate representation of the unique features of each anime grouping. ","version":"Next","tagName":"h3"},{"title":"📏 Normalizing Data","type":0,"sectionRef":"#","url":"/kelbrum/normalize/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"📏 Normalizing Data for K-means clustering​","type":1,"pageTitle":"📏 Normalizing Data","url":"/kelbrum/normalize/#-normalizing-data-for-k-means-clustering","content":" Given the strucure of an AnimeEntry model, it can be seen that there is a variety of data types present. There is data that can be considered as categorical, ordinal, and numerical.  To handle these various data types and ensure that K-means clustering was possible, there was a need to normalize the existing data. Various normalization techniques were implemented and after trial and error, the following were used in the current model:  Ordinal Encoding For anime properties such as type, rating, and demographics, originally I was using multi-hot encoding however, I noticed that just the presence/absence of a property was not sufficient as the distance between anime was key. For example, I wanted anime of type TV and ONA to be closer in distance compared to TV and Movie. To accomplish this, I chose to assign distinct values to each unique value within each of these properties, leveraging my understanding of how related values are associated with each otherMulti-Hot Encoding: For anime properties such as genres, demographics, source, etc, this was normalized into a binary vector filled with 1's and 0's, indicating the presence or absence of a unique valueMin-Max Scaling: For anime properties such as score and durationMinutes, the data was normalized by determining the minimum and maximum values of each property and utilizing the following formula: (value - min) / (max - min)**Robust Scaling: This method could be applied to numerical attributes like score and episodes, which would be beneficial given the presence of outliers in the dataset. These outliers include anime with scores or episode counts of 0 or &quot;Unknown&quot; due to insufficient informationTF-IDF: This was primarily used for the synopsis, utilzing natural, remove-stopwords, word-list, and lemmatizer in combination, the goal was to extract top keywords from each anime synopsis and determine their frequency. In the current state, the frequency value is not used as is and instead only the presence/absence represented by a 1 or 0 is used. Originally, I planned to use the top 10 keywords from each anime and compare it with word-list, however, this did result in words I felt were irrelevant to the synopsis. To combat this, I created a list of keywords which started by using the genres, themes, demographics as a base and looking at popular anime in combination with the top 1000 keywords returned using the word-list approach and adding what I felt was relevant. There is definetly a lot of keywords that can be added and further improvements are possible.  Other normalization techniques were explored as well during the experimentation phase of the project, such as:  Combination Encoding: This technique aims to represent each unique value with a distinct integer. For each anime, the values of its properties are combined into a single value. For instance, consider the genres property of a given anime with values ['action', 'comedy', 'romance']. Each genre is assigned a unique integer, starting from 0 up to the total count of unique values. These integers are then summed to produce a single value for the genres property of that anime  To see more information about the normalization process, you can consider checking out the source code which shows all the normalization functions and additionally, how they are incorporated to create feature tensors using Tensorflow. ","version":"Next","tagName":"h2"},{"title":"🧩 Model Overview","type":0,"sectionRef":"#","url":"/kelbrum/model/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"🧩 Model Overview​","type":1,"pageTitle":"🧩 Model Overview","url":"/kelbrum/model/#-model-overview","content":" AnimeEntry: This is the main model which is meant to represent a distinct anime entry, including properties such as its title, type, genres, studios, score, pageURL, etc.  This is the primary and at present, the only model used within this project. Every entry within the original dataset is read row by row and processed into this model.  An example of an AnimeEntry model can be seen below:   &quot;id&quot;: 5183, &quot;malID&quot;: 11061, &quot;title&quot;: &quot;Hunter x Hunter (2011)&quot;, &quot;englishName&quot;: &quot;Hunter x Hunter&quot;, &quot;otherName&quot;: &quot;HUNTER×HUNTER（ハンター×ハンター）&quot;, &quot;titles&quot;: [ &quot;Hunter x Hunter&quot;, &quot;HxH&quot;, &quot;HUNTER×HUNTER（ハンター×ハンター）&quot;, &quot;Hunter X Hunter: Cazadores de Tesoros&quot; ], &quot;score&quot;: 9.04, &quot;genres&quot;: [ &quot;Action&quot;, &quot;Adventure&quot;, &quot;Fantasy&quot; ], &quot;themes&quot;: [], &quot;demographics&quot;: [ &quot;Shounen&quot; ], &quot;synopsis&quot;: &quot;Hunters devote themselves to accomplishing hazardous tasks, all from traversing the world's uncharted territories to locating rare items and monsters. Before becoming a Hunter, one must pass the Hunter Examination—a high-risk selection process in which most applicants end up handicapped or worse, deceased.\\n\\nAmbitious participants who challenge the notorious exam carry their own reason. What drives 12-year-old Gon Freecss is finding Ging, his father and a Hunter himself. Believing that he will meet his father by becoming a Hunter, Gon takes the first step to walk the same path.\\n\\nDuring the Hunter Examination, Gon befriends the medical student Leorio Paladiknight, the vindictive Kurapika, and ex-assassin Killua Zoldyck. While their motives vastly differ from each other, they band together for a common goal and begin to venture into a perilous world.&quot;, &quot;type&quot;: &quot;TV&quot;, &quot;episodes&quot;: 148, &quot;aired&quot;: &quot;Oct 2, 2011 to Sep 24, 2014&quot;, &quot;premiered&quot;: &quot;fall 2011&quot;, &quot;season&quot;: &quot;fall&quot;, &quot;year&quot;: 2011, &quot;status&quot;: &quot;Finished Airing&quot;, &quot;producers&quot;: [ &quot;Nippon Television Network&quot;, &quot;Shueisha&quot;, &quot;VAP&quot; ], &quot;licensors&quot;: [ &quot;VIZ Media&quot; ], &quot;studios&quot;: [ &quot;Madhouse&quot; ], &quot;source&quot;: &quot;Manga&quot;, &quot;durationText&quot;: &quot;23 min per ep&quot;, &quot;durationMinutes&quot;: 23, &quot;rating&quot;: &quot;PG-13&quot;, &quot;rank&quot;: 10, &quot;popularity&quot;: 10, &quot;favourites&quot;: 200265, &quot;scoredBy&quot;: 1651790, &quot;members&quot;: 2656870, &quot;imageURL&quot;: &quot;https://cdn.myanimelist.net/images/anime/1337/99013.jpg&quot;, &quot;trailerURL&quot;: &quot;Unknown&quot;, &quot;pageURL&quot;: &quot;https://myanimelist.net/anime/11061/Hunter x Hunter (2011)&quot;,   For the purposes of the recommendation system, not all properties are applicable. Through the development process, the following properties were used:  scoregenresthemesdemographicstypesourcedurationMinutesratingsynopsisyear  There is definetly room for improvement especially through the incorporation of more properties such as the studios, favourites, members, rank, popularity, and episodes. To further improve the recommendations, genres, themes should use a form of ordinal encoding similar to type, demographics, and rating assigning unique numerical values to each possible value and trying to group similar properties like action and adventure closer together than for example, action and comedy. It would also be nice to have additional unexplored properties such as countryOfOrigin which can be used to group similar anime based on where the source originated from. ","version":"Next","tagName":"h2"},{"title":"🛠️ Tech Stack","type":0,"sectionRef":"#","url":"/kelbrum/stack/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"🛠️ Tech Stack​","type":1,"pageTitle":"🛠️ Tech Stack","url":"/kelbrum/stack/#️-tech-stack","content":" Backend:  Node.jsTensorflow.jsnaturalsimple-statisticsml-distanceml-kmeans  Frontend:  ReactReact RouterDaisyUITailwindCSS  Hosting:  Firebase Analytics using Google Analytics (Based on recommended Firebase config)  Documentation:  Docs are built using Docusaurus Search functionality provided by: docusaurus-lunr-searchAnalytics using Google Analytics Documentation site hosted via GitHub Pages  Dev Tools:  ESLintPrettierWakaTime ","version":"Next","tagName":"h2"},{"title":"⚡ Setup","type":0,"sectionRef":"#","url":"/kelbrum/setup/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"⚡ Setup Instructions​","type":1,"pageTitle":"⚡ Setup","url":"/kelbrum/setup/#-setup-instructions","content":" Clone this repository to your local machine.  git clone https://github.com/vikiru/kelbrum.git cd kelbrum   Download and install all required dependencies.  npm install  ","version":"Next","tagName":"h2"},{"title":"📝 Prerequisites","type":0,"sectionRef":"#","url":"/kelbrum/prerequisites/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"📝 Prerequisites​","type":1,"pageTitle":"📝 Prerequisites","url":"/kelbrum/prerequisites/#-prerequisites","content":" Ensure that the following dependencies are installed onto your machine by following the Setup Instructions.  Node.js ","version":"Next","tagName":"h2"},{"title":"🔧 Development Overview","type":0,"sectionRef":"#","url":"/kelbrum/development/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"🔧 Development Overview​","type":1,"pageTitle":"🔧 Development Overview","url":"/kelbrum/development/#-development-overview","content":" To develop a project like this, several different steps were neccessary alongside many hours of trial and error.  ","version":"Next","tagName":"h2"},{"title":"Finding a suitable database​","type":1,"pageTitle":"🔧 Development Overview","url":"/kelbrum/development/#finding-a-suitable-database","content":" To achieve this, I browsed through Kaggle to find datasets with the information that I would need for this situation, ideally just the anime information would have been enough. I originally used this dataset however, I ended up transitioning to the current dataset as it provided more detailed information and addtionally, I could see the license information for the dataset.  ","version":"Next","tagName":"h3"},{"title":"Determining how to use dataset​","type":1,"pageTitle":"🔧 Development Overview","url":"/kelbrum/development/#determining-how-to-use-dataset","content":" My first thought was to figure out how to use machine learning frameworks such as Tensorflow or PyTorch and create a model based on the dataset using those. This would have involved a similar kind of process to what I achieved in the end, normalizing the data and creating feature tensors, however, I would have needed labels to test the models ability to predict. I could have used the score property of an anime however, there are several outliers in the thousands with values of 0 or 'Unknown' for various properties including score and additionally, this did not suit my use case. I later learned that what I was originally trying to achieve was something known as supervised learning and then I learned that there was something known as unsupervised learning which did not need labels of any kind, this seemed like what I needed.  From here, I had to determine the type of filtering to use within the recommendation system, I learned there were two main types, content-based and collaborative filtering followed by a hybrid approach which would combine both of the methods. Given the dataset, I did have the capability to perform both types of filtering, however, the size of the user interaction data was 1 GB and I did attempt to try to use it however, the issue is that there are over 200,000 users within that data and over 15,000 anime within the dataset which means utilizing this in a user-interaction matrix would have been computationally expensive. That is when I decided to just use content-based filtering.  ","version":"Next","tagName":"h3"},{"title":"Normalizing the Data​","type":1,"pageTitle":"🔧 Development Overview","url":"/kelbrum/development/#normalizing-the-data","content":" Once I determined that I would use content-based filtering, I had to research various normalizing techniques for the various types of data that existed within my data and use this to create feature tensors through Tensorflow, further explained here.  ","version":"Next","tagName":"h3"},{"title":"Clustering the Data​","type":1,"pageTitle":"🔧 Development Overview","url":"/kelbrum/development/#clustering-the-data","content":" Since the objective of my project was to recommend anime that was similar to user-selected anime, I had to figure out a way to group anime based on their properties. This led me to research various existing clustering algorithms where I finalized on using k-means clustering. To effectively cluster anime, I had to also determine a distance function which would be used to tell how similar two distinct anime were based on their properies, in the end, a combination of the Manhattan and Dice distance was used. This process is further explained here.  ","version":"Next","tagName":"h3"},{"title":"Designing the UI​","type":1,"pageTitle":"🔧 Development Overview","url":"/kelbrum/development/#designing-the-ui","content":" Once all the previous steps were completed, the final step was to design a simple and easy to use UI that was as visually appealing as possible, to achieve this goal I utilized React and React Router in combination with TailwindCSS and DaisyUI alongside various npm packages such as MiniSearch, lodash, tailwind-scrollbar, React Infinite Scroller, react-slick, and slick-carousel. ","version":"Next","tagName":"h3"},{"title":"📜 Available Scripts","type":0,"sectionRef":"#","url":"/kelbrum/scripts/","content":"","keywords":"Kelbrum  anime  recommendation  engine  system  machine learning  tensorflow  similarity  kmeans  react  react router  tailwindcss  daisyui","version":"Next"},{"title":"📜 Available Scripts​","type":1,"pageTitle":"📜 Available Scripts","url":"/kelbrum/scripts/#-available-scripts","content":" Start the app in the development environment.  npm start   Build the project files and optimize for production.  npm run build   Lint all files and check if there are any issues, with ESLint.  npm run lint   Fix all ESLint issues then format the files with Prettier.  npm run prettier  ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}